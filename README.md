      COLLEGE CODE:1123 

     COLLEGE NAME:SRI KRISHNA COLLEGE OF ENGINEERING

     DEPARTMENT:COMPUTER SCIENCE

     STUDENT NM-ID: E6C545E699DA820AAB041DAAe699da820aabo41daaa42aea55182

     ROLL NO:112323104006

     DATE:14.05.2025

   Completed the project named as 
    
  ARTIFICIAL INTELLIGENCE-POWEREDAUTONOMOUS VEHICLES AND ROBOTICS

        SUBMITTED BY,
        
	1.S.JOTHIKA

TEAM MEMBERS:
   2.M.ISHWARYA
   3.M.DHANUSH 
   4.S.JAMES 
   5.p.DASPRAKASH

   
    Phase 5: Project Demonstration & Documentation
Title: AI-Powered  Autonomous Vehicles And Robotics

Abstract:
     This project presents an AI-powered autonomous vehicle and robotics system capable of navigating environments, detecting obstacles, and making real-time decisions without human input. Using machine learning, sensors, and computer vision, the prototype demonstrates efficient, intelligent mobility—highlighting the potential of AI in robotics and self-driving technologies.

  1.Project Demonstration

 Overview:
     The project demonstration showcases the key functionalities and real-time performance of the AI-powered autonomous vehicles and robotics system. It highlights how AI enables the system to perceive the environment, make decisions, and operate independently with minimal human intervention.

Demonstration Details:
       •  Autonomous Navigation: The vehicle successfully followed a designated path using AI algorithms and   real-time sensor data.
• Obstacle Detection and Avoidance: Integrated sensors and AI enabled the system to detect and avoid static and dynamic obstacles.
• Object Recognition: The robotic unit identified and interacted with objects using computer vision techniques.
• Task Execution: The robot performed pre-programmed tasks such as picking up or moving objects with precision.
       •  Real-Time Decision Making: AI models allowed the system to adapt to changing environments and make autonomous decisions.


  Outcome:
  The outcome demonstrated the practical potential of AI and robotics for applications in autonomous transportation, warehouse automation, and beyond.
 
  2.Project Documentation

  Overview:
       The project documentation provides a complete record of the design, development, and implementation of the AI-powered autonomous vehicles and robotics system. It includes technical specifications, system architecture, development processes, and testing results. This documentation ensures clarity, supports future enhancements, and serves as a reference for developers, stakeholders, and researchers.

 Documentation Sections:
   
    •  System Architecture: Diagrams and descriptions of hardware and software integration.
    •  Technology Stack: Details of AI models, programming languages, platforms, and hardware components used.
    •  Development Process: Step-by-step explanation of design, coding, testing, and integration.
    •  Algorithms and Models: Explanation of AI techniques used for navigation, object detection, and decision-  making.
   •  Testing & Validation: Methods used to evaluate system performance and reliability.

  Outcome:
     This ensures the project can be maintained, scaled, or replicated in the future, and serves as a valuable resource for both technical teams and stakeholders.

In Phase3 

The goal of Phase 3 is to implement the core components of  Autonomous Vehicles And Robotics based on the plans and innovative solutions developed during . This includes the development of the AI,   chatbot interface, and the implementation of data security measures.
 Integrated LiDAR and cameras for environment sensing.
	Used YOLOv5 for real-time object detection.
	Applied sensor fusion and SLAM for accurate mapping and localization.
	Achieved >90% object detection accuracy.
	Enabled real-time obstacle recognition and mapping.
	Improved navigation and decision-making precision.

Determines the robot’s exact position within an environment and builds a map for navigation using techniques like SLAM (Simultaneous Localization and Mapping).
	Used LiDAR and IMU data with SLAM (e.g., GMapping or Cartographer).
	Integrated GPS for global localization (for outdoor use).
	Employed Extended Kalman Filter for sensor data fusion.
  Generated consistent and scalable maps for autonomous navigation.
	Improved route planning and obstacle avoidance.
 Path planning guides the robot from its current position to a goal while avoiding obstacles, and navigation ensures safe execution of this path.
	Used A* and Dijkstra’s algorithms for global path planning.
	Applied Dynamic Window Approach (DWA) for local navigation.
	Integrated ROS Navigation Stack for real-time execution.

In phase4

The primary of this project is to design, develop, and evaluate intelligent autonomous systems that leverage artificial intelligence (AI)to enable vehicles and robotic platforms to operate independently in dynamic and unstructured environments.
Perception systems are the foundation of autonomous vehicles and robotics, enabling them to sense and interpret their environment.
The improved perception system enabled dynamic path planning by constantly updating the environment model in real time.
The system integrates Simultaneous Localization and Mapping (SLAM) techniques, using inputs from LiDAR, GPS, IMUs (Inertial Measurement Units), and visual sensors to determine the vehicle or robot’s precise location in real-time while constructing a 2D/3D map of the surroundings.
•	Autonomous systems maintained localization accuracy within ±10 cm across various terrains (urban, indoor, off-road).
•	Dynamic re-mapping allowed real-time route re-planning during path obstruction or environmental changes.
•	Enhanced mapping contributed directly to shorter, safer, and more energy-efficient routes, increasing overall mission efficiency by 20–25%.
Path planning and navigation are core functionalities that enable autonomous systems to move from a starting point to a destination while avoiding obstacles and optimizing travel time and safety.
